{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aeb6abb-9174-4173-8e50-a87289bdf1f6",
   "metadata": {},
   "source": [
    "# Programming in PySpark RDDâ€™s\n",
    "\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is the fundamental and backbone data type of this engine. This track introduces RDDs and shows how RDDs can be created and executed using RDD Transformations and Actions.\n",
    "\n",
    "## Preparing the environment\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb83892-ecb4-4a80-9ed4-051092ed3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "\n",
    "from pyspark.sql.types import (_parse_datatype_string, StructType, StructField,\n",
    "                               DoubleType, IntegerType, StringType)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd77504-11cd-47ba-9700-96338f4faecb",
   "metadata": {},
   "source": [
    "### Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e464850-a646-4eb2-b8b5-9617652429c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# eval DataFrame in notebooks\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc82654-89a9-4ad2-81d6-be6efd86cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfab86a-817f-462b-8ca6-d3605f3cba00",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d619642b-2bd7-4396-98c2-c9248c689983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Photo: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Flag: string (nullable = true)\n",
      " |-- Overall: integer (nullable = true)\n",
      " |-- Potential: integer (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- Club Logo: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Wage: string (nullable = true)\n",
      " |-- Special: integer (nullable = true)\n",
      " |-- Acceleration: integer (nullable = true)\n",
      " |-- Aggression: integer (nullable = true)\n",
      " |-- Agility: integer (nullable = true)\n",
      " |-- Balance: integer (nullable = true)\n",
      " |-- Ball control: integer (nullable = true)\n",
      " |-- Composure: integer (nullable = true)\n",
      " |-- Crossing: integer (nullable = true)\n",
      " |-- Curve: integer (nullable = true)\n",
      " |-- Dribbling: integer (nullable = true)\n",
      " |-- Finishing: integer (nullable = true)\n",
      " |-- Free kick accuracy: integer (nullable = true)\n",
      " |-- GK diving: integer (nullable = true)\n",
      " |-- GK handling: integer (nullable = true)\n",
      " |-- GK kicking: integer (nullable = true)\n",
      " |-- GK positioning: integer (nullable = true)\n",
      " |-- GK reflexes: integer (nullable = true)\n",
      " |-- Heading accuracy: integer (nullable = true)\n",
      " |-- Interceptions: integer (nullable = true)\n",
      " |-- Jumping: integer (nullable = true)\n",
      " |-- Long passing: integer (nullable = true)\n",
      " |-- Long shots: integer (nullable = true)\n",
      " |-- Marking: integer (nullable = true)\n",
      " |-- Penalties: integer (nullable = true)\n",
      " |-- Positioning: integer (nullable = true)\n",
      " |-- Reactions: integer (nullable = true)\n",
      " |-- Short passing: integer (nullable = true)\n",
      " |-- Shot power: integer (nullable = true)\n",
      " |-- Sliding tackle: integer (nullable = true)\n",
      " |-- Sprint speed: integer (nullable = true)\n",
      " |-- Stamina: integer (nullable = true)\n",
      " |-- Standing tackle: integer (nullable = true)\n",
      " |-- Strength: integer (nullable = true)\n",
      " |-- Vision: integer (nullable = true)\n",
      " |-- Volleys: integer (nullable = true)\n",
      " |-- CAM: double (nullable = true)\n",
      " |-- CB: double (nullable = true)\n",
      " |-- CDM: double (nullable = true)\n",
      " |-- CF: double (nullable = true)\n",
      " |-- CM: double (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LAM: double (nullable = true)\n",
      " |-- LB: double (nullable = true)\n",
      " |-- LCB: double (nullable = true)\n",
      " |-- LCM: double (nullable = true)\n",
      " |-- LDM: double (nullable = true)\n",
      " |-- LF: double (nullable = true)\n",
      " |-- LM: double (nullable = true)\n",
      " |-- LS: double (nullable = true)\n",
      " |-- LW: double (nullable = true)\n",
      " |-- LWB: double (nullable = true)\n",
      " |-- Preferred Positions: string (nullable = true)\n",
      " |-- RAM: double (nullable = true)\n",
      " |-- RB: double (nullable = true)\n",
      " |-- RCB: double (nullable = true)\n",
      " |-- RCM: double (nullable = true)\n",
      " |-- RDM: double (nullable = true)\n",
      " |-- RF: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- RS: double (nullable = true)\n",
      " |-- RW: double (nullable = true)\n",
      " |-- RWB: double (nullable = true)\n",
      " |-- ST: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>Name</th><th>Age</th><th>Photo</th><th>Nationality</th><th>Flag</th><th>Overall</th><th>Potential</th><th>Club</th><th>Club Logo</th><th>Value</th><th>Wage</th><th>Special</th><th>Acceleration</th><th>Aggression</th><th>Agility</th><th>Balance</th><th>Ball control</th><th>Composure</th><th>Crossing</th><th>Curve</th><th>Dribbling</th><th>Finishing</th><th>Free kick accuracy</th><th>GK diving</th><th>GK handling</th><th>GK kicking</th><th>GK positioning</th><th>GK reflexes</th><th>Heading accuracy</th><th>Interceptions</th><th>Jumping</th><th>Long passing</th><th>Long shots</th><th>Marking</th><th>Penalties</th><th>Positioning</th><th>Reactions</th><th>Short passing</th><th>Shot power</th><th>Sliding tackle</th><th>Sprint speed</th><th>Stamina</th><th>Standing tackle</th><th>Strength</th><th>Vision</th><th>Volleys</th><th>CAM</th><th>CB</th><th>CDM</th><th>CF</th><th>CM</th><th>ID</th><th>LAM</th><th>LB</th><th>LCB</th><th>LCM</th><th>LDM</th><th>LF</th><th>LM</th><th>LS</th><th>LW</th><th>LWB</th><th>Preferred Positions</th><th>RAM</th><th>RB</th><th>RCB</th><th>RCM</th><th>RDM</th><th>RF</th><th>RM</th><th>RS</th><th>RW</th><th>RWB</th><th>ST</th></tr>\n",
       "<tr><td>0</td><td>Cristiano Ronaldo</td><td>32</td><td>https://cdn.sofif...</td><td>Portugal</td><td>https://cdn.sofif...</td><td>94</td><td>94</td><td>Real Madrid CF</td><td>https://cdn.sofif...</td><td>&euro;95.5M</td><td>&euro;565K</td><td>2228</td><td>89</td><td>63</td><td>89</td><td>63</td><td>93</td><td>95</td><td>85</td><td>81</td><td>91</td><td>94</td><td>76</td><td>7</td><td>11</td><td>15</td><td>14</td><td>11</td><td>88</td><td>29</td><td>95</td><td>77</td><td>92</td><td>22</td><td>85</td><td>95</td><td>96</td><td>83</td><td>94</td><td>23</td><td>91</td><td>92</td><td>31</td><td>80</td><td>85</td><td>88</td><td>89.0</td><td>53.0</td><td>62.0</td><td>91.0</td><td>82.0</td><td>20801</td><td>89.0</td><td>61.0</td><td>53.0</td><td>82.0</td><td>62.0</td><td>91.0</td><td>89.0</td><td>92.0</td><td>91.0</td><td>66.0</td><td>ST LW </td><td>89.0</td><td>61.0</td><td>53.0</td><td>82.0</td><td>62.0</td><td>91.0</td><td>89.0</td><td>92.0</td><td>91.0</td><td>66.0</td><td>92.0</td></tr>\n",
       "<tr><td>1</td><td>L. Messi</td><td>30</td><td>https://cdn.sofif...</td><td>Argentina</td><td>https://cdn.sofif...</td><td>93</td><td>93</td><td>FC Barcelona</td><td>https://cdn.sofif...</td><td>&euro;105M</td><td>&euro;565K</td><td>2154</td><td>92</td><td>48</td><td>90</td><td>95</td><td>95</td><td>96</td><td>77</td><td>89</td><td>97</td><td>95</td><td>90</td><td>6</td><td>11</td><td>15</td><td>14</td><td>8</td><td>71</td><td>22</td><td>68</td><td>87</td><td>88</td><td>13</td><td>74</td><td>93</td><td>95</td><td>88</td><td>85</td><td>26</td><td>87</td><td>73</td><td>28</td><td>59</td><td>90</td><td>85</td><td>92.0</td><td>45.0</td><td>59.0</td><td>92.0</td><td>84.0</td><td>158023</td><td>92.0</td><td>57.0</td><td>45.0</td><td>84.0</td><td>59.0</td><td>92.0</td><td>90.0</td><td>88.0</td><td>91.0</td><td>62.0</td><td>RW </td><td>92.0</td><td>57.0</td><td>45.0</td><td>84.0</td><td>59.0</td><td>92.0</td><td>90.0</td><td>88.0</td><td>91.0</td><td>62.0</td><td>88.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+--------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n",
       "|_c0|             Name|Age|               Photo|Nationality|                Flag|Overall|Potential|          Club|           Club Logo| Value| Wage|Special|Acceleration|Aggression|Agility|Balance|Ball control|Composure|Crossing|Curve|Dribbling|Finishing|Free kick accuracy|GK diving|GK handling|GK kicking|GK positioning|GK reflexes|Heading accuracy|Interceptions|Jumping|Long passing|Long shots|Marking|Penalties|Positioning|Reactions|Short passing|Shot power|Sliding tackle|Sprint speed|Stamina|Standing tackle|Strength|Vision|Volleys| CAM|  CB| CDM|  CF|  CM|    ID| LAM|  LB| LCB| LCM| LDM|  LF|  LM|  LS|  LW| LWB|Preferred Positions| RAM|  RB| RCB| RCM| RDM|  RF|  RM|  RS|  RW| RWB|  ST|\n",
       "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+--------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n",
       "|  0|Cristiano Ronaldo| 32|https://cdn.sofif...|   Portugal|https://cdn.sofif...|     94|       94|Real Madrid CF|https://cdn.sofif...|â‚¬95.5M|â‚¬565K|   2228|          89|        63|     89|     63|          93|       95|      85|   81|       91|       94|                76|        7|         11|        15|            14|         11|              88|           29|     95|          77|        92|     22|       85|         95|       96|           83|        94|            23|          91|     92|             31|      80|    85|     88|89.0|53.0|62.0|91.0|82.0| 20801|89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|             ST LW |89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|92.0|\n",
       "|  1|         L. Messi| 30|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     93|       93|  FC Barcelona|https://cdn.sofif...| â‚¬105M|â‚¬565K|   2154|          92|        48|     90|     95|          95|       96|      77|   89|       97|       95|                90|        6|         11|        15|            14|          8|              71|           22|     68|          87|        88|     13|       74|         93|       95|           88|        85|            26|          87|     73|             28|      59|    90|     85|92.0|45.0|59.0|92.0|84.0|158023|92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|                RW |92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|88.0|\n",
       "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+--------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifa = spark.read.csv('data-sources/Fifa2018_dataset.csv', header=True, inferSchema=True)\n",
    "# cast to integer\n",
    "for col_name in ['Acceleration', 'Aggression', 'Agility', 'Balance', 'Ball control', 'Composure', \n",
    "                 'Crossing', 'Curve', 'Dribbling', 'Finishing', 'Free kick accuracy', 'GK diving', \n",
    "                 'GK handling', 'GK kicking', 'GK positioning', 'GK reflexes', 'Heading accuracy', \n",
    "                 'Interceptions', 'Jumping', 'Long passing', 'Long shots', 'Marking', 'Penalties', \n",
    "                 'Positioning', 'Reactions', 'Short passing', 'Shot power', 'Sliding tackle', \n",
    "                 'Sprint speed', 'Stamina', 'Standing tackle', 'Strength', 'Vision', 'Volleys']:\n",
    "    fifa = fifa.withColumn(col_name, fifa[col_name].cast('integer'))\n",
    "fifa.createOrReplaceTempView(\"fifa\")\n",
    "fifa.printSchema()\n",
    "fifa.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f404b20-7b00-4c76-b025-76375fc89c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>userId</th><th>movieId</th><th>rating</th><th>timestamp</th></tr>\n",
       "<tr><td>1</td><td>31</td><td>2.5</td><td>1260759144</td></tr>\n",
       "<tr><td>1</td><td>1029</td><td>3.0</td><td>1260759179</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+-------+------+----------+\n",
       "|userId|movieId|rating| timestamp|\n",
       "+------+-------+------+----------+\n",
       "|     1|     31|   2.5|1260759144|\n",
       "|     1|   1029|   3.0|1260759179|\n",
       "+------+-------+------+----------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = spark.read.csv('data-sources/movie-ratings.csv', header=False, inferSchema=True,\n",
    "                        schema='userId int, movieId int, rating double, timestamp int')\n",
    "movies.createOrReplaceTempView(\"movies\")\n",
    "movies.printSchema()\n",
    "movies.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a62bc0cc-b634-40ba-b47f-9c0c13247655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- date of birth: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>person_id</th><th>name</th><th>sex</th><th>date of birth</th></tr>\n",
       "<tr><td>0</td><td>100</td><td>Penelope Lewis</td><td>female</td><td>1990-08-31 00:00:00</td></tr>\n",
       "<tr><td>1</td><td>101</td><td>David Anthony</td><td>male</td><td>1971-10-14 00:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+--------------+------+-------------------+\n",
       "|_c0|person_id|          name|   sex|      date of birth|\n",
       "+---+---------+--------------+------+-------------------+\n",
       "|  0|      100|Penelope Lewis|female|1990-08-31 00:00:00|\n",
       "|  1|      101| David Anthony|  male|1971-10-14 00:00:00|\n",
       "+---+---------+--------------+------+-------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = spark.read.csv('data-sources/people.csv', header=True, inferSchema=True)\n",
    "people.createOrReplaceTempView(\"people\")\n",
    "people.printSchema()\n",
    "people.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac7ae33-bbcd-4a1e-9496-2ef4a0a4002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Wine: integer (nullable = true)\n",
      " |-- Alcohol: double (nullable = true)\n",
      " |-- Malic.acid: double (nullable = true)\n",
      " |-- Ash: double (nullable = true)\n",
      " |-- Acl: double (nullable = true)\n",
      " |-- Mg: integer (nullable = true)\n",
      " |-- Phenols: double (nullable = true)\n",
      " |-- Flavanoids: double (nullable = true)\n",
      " |-- Nonflavanoid.phenols: double (nullable = true)\n",
      " |-- Proanth: double (nullable = true)\n",
      " |-- Color.int: double (nullable = true)\n",
      " |-- Hue: double (nullable = true)\n",
      " |-- OD: double (nullable = true)\n",
      " |-- Proline: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Wine</th><th>Alcohol</th><th>Malic.acid</th><th>Ash</th><th>Acl</th><th>Mg</th><th>Phenols</th><th>Flavanoids</th><th>Nonflavanoid.phenols</th><th>Proanth</th><th>Color.int</th><th>Hue</th><th>OD</th><th>Proline</th></tr>\n",
       "<tr><td>1</td><td>14.23</td><td>1.71</td><td>2.43</td><td>15.6</td><td>127</td><td>2.8</td><td>3.06</td><td>0.28</td><td>2.29</td><td>5.64</td><td>1.04</td><td>3.92</td><td>1065</td></tr>\n",
       "<tr><td>1</td><td>13.2</td><td>1.78</td><td>2.14</td><td>11.2</td><td>100</td><td>2.65</td><td>2.76</td><td>0.26</td><td>1.28</td><td>4.38</td><td>1.05</td><td>3.4</td><td>1050</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-------+----------+----+----+---+-------+----------+--------------------+-------+---------+----+----+-------+\n",
       "|Wine|Alcohol|Malic.acid| Ash| Acl| Mg|Phenols|Flavanoids|Nonflavanoid.phenols|Proanth|Color.int| Hue|  OD|Proline|\n",
       "+----+-------+----------+----+----+---+-------+----------+--------------------+-------+---------+----+----+-------+\n",
       "|   1|  14.23|      1.71|2.43|15.6|127|    2.8|      3.06|                0.28|   2.29|     5.64|1.04|3.92|   1065|\n",
       "|   1|   13.2|      1.78|2.14|11.2|100|   2.65|      2.76|                0.26|   1.28|     4.38|1.05| 3.4|   1050|\n",
       "+----+-------+----------+----+----+---+-------+----------+--------------------+-------+---------+----+----+-------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = spark.read.csv('data-sources/wine-data.csv', header=True, inferSchema=True)\n",
    "wine.createOrReplaceTempView(\"wine\")\n",
    "wine.printSchema()\n",
    "wine.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19044beb-87d2-43b0-ab76-009b5fb276f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='fifa', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='movies', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='people', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='wine', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b51fb7-1396-4730-b277-d4e7262bfdec",
   "metadata": {},
   "source": [
    "## Ex. 1 - RDDs from Parallelized collections\n",
    "Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it. In this exercise, you'll create your first RDD in PySpark from a collection of words.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a RDD named `RDD` from a Python list of words.\n",
    "2. Confirm the object created is RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34688daf-c00a-4384-98fb-d610addc8980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ParallelCollectionRDD[69] at readRDDFromFile at PythonRDD.scala:289\n",
      "Total values: 6\n",
      "Values:\n",
      "['Spark', 'is', 'a', 'framework', 'for', 'Big Data processing']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Review the parallelized data\n",
    "print(f\"\"\"\n",
    "{RDD}\n",
    "Total values: {RDD.count()}\n",
    "Values:\n",
    "{RDD.take(6)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa92b5d-6c0f-45fb-8e6d-ef5846d60e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8da64e-8764-46d9-bb49-744bac89a5ea",
   "metadata": {},
   "source": [
    "## Ex. 2 - RDDs from External Datasets\n",
    "\n",
    "PySpark can easily create RDDs from files that are stored in external storage devices, such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines. In this exercise, you'll create an RDD from the file path (`file_path`) with the file name `sample_text.md` which is already available in your workspace.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Print the `file_path` in the PySpark shell.\n",
    "2. Create a RDD named `fileRDD` from a file_path.\n",
    "3. Print the type of the `fileRDD` created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5049b5f4-34b3-4827-8909-91d577ccd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file_path is data-sources/sample_text.md\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data-sources/sample_text.md'\n",
    "\n",
    "# Print the file_path\n",
    "print(\"The file_path is\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e7b0c0-57ca-4b08-bc9e-bb84fad1eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data-sources/sample_text.md MapPartitionsRDD[75] at textFile at <unknown>:0\n",
      "Total lines in the file: 36\n",
      "First 5 lines of the file:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[![buildstatus](https://travis-ci.org/holdenk/learning-spark-examples.svg?branch=master)](https://travis-ci.org/holdenk/learning-spark-examples)',\n",
       " 'Examples for Learning Spark',\n",
       " '===============',\n",
       " 'Examples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file',\n",
       " 'in the mini-complete-example directory.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Review the loaded data\n",
    "print(f\"\"\"\n",
    "{fileRDD}\n",
    "Total lines in the file: {fileRDD.count()}\n",
    "First 5 lines of the file:\n",
    "\"\"\")\n",
    "fileRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99db0cf4-5c0f-4e14-ab3a-6bac9391bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bc209-7f94-4ec1-8a81-bf6c6e507cc2",
   "metadata": {},
   "source": [
    "## Ex. 3 - Partitions in your data\n",
    "\n",
    "SparkContext's `textFile()` method takes an optional second argument called `minPartitions` for specifying the minimum number of partitions. In this exercise, you'll create a RDD named `fileRDD_part` with 5 partitions and then compare that with fileRDD that you created in the previous exercise. Refer to the \"Understanding Partition\" slide in video 2.1 to know the methods for creating and getting the number of partitions in a RDD.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find the number of partitions that support `fileRDD` RDD.\n",
    "2. Create an RDD named `fileRDD_part` from the file path but create 5 partitions.\n",
    "3. Confirm the number of partitions in the new `fileRDD_part` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1825dae-2449-4df2-9eb1-eafbe2627789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD is 2\n",
      "Number of partitions in fileRDD_part is 5\n"
     ]
    }
   ],
   "source": [
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44947f-0da3-483a-87b9-c0d55cc9bae4",
   "metadata": {},
   "source": [
    "## Basic RDD Transformations and Actions\n",
    "\n",
    "### map() - RDD Transformations\n",
    "\n",
    "`map()` transformation applies a function to all elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db681d61-db18-4921-870c-778c2480c65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = sc.parallelize([1,2,3,4])\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e01a04-829d-41f6-b510-041cf0ff4ced",
   "metadata": {},
   "source": [
    "### filter() - RDD Transformations\n",
    "\n",
    "`filter()` transformation returns a new RDD with only the elements that pass the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb40565a-5062-47f3-bbe9-48f48016bbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = sc.parallelize([1,2,3,4])\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de3a2b-4005-477d-9ebb-df7029db6c24",
   "metadata": {},
   "source": [
    "### flatMap() - RDD Transformations\n",
    "\n",
    "`flatMap()` transformation returns multiple values for each element in the original RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d31cc24-6a1c-43f5-9b65-789920184d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "RDD_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c5153-72e9-4988-849c-0898bead0abe",
   "metadata": {},
   "source": [
    "### union() - RDD Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "165b297b-623f-4858-83bb-8896198484e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputRDD = sc.textFile(\"data-sources/data.log\")\n",
    "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split())\n",
    "warningsRDD = inputRDD.filter(lambda x: \"warning\" in x.split())\n",
    "combinedRDD = errorRDD.union(warningsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b22a56a-5a96-4db2-a234-ecfe2526ff26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03/22 08:51:06 error   :...read_physical_netif: index #3, interface LINK12 has address 9.67.101.1, ifidx 3',\n",
       " '03/22 08:51:06 error   :...read_physical_netif: index #5, interface CTCD2 has address 9.67.117.98, ifidx 5']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58e9244f-7454-4746-b0b5-8f28a73f2f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03/22 08:51:06 warning :...read_physical_netif: index #3, interface LINK12 has address 9.67.101.1, ifidx 3',\n",
       " '03/22 08:51:06 warning :...read_physical_netif: index #6, interface LOOPBACK has address 127.0.0.1, ifidx 0']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warningsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad8315b9-d347-49a3-849a-d68dee133d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03/22 08:51:06 error   :...read_physical_netif: index #3, interface LINK12 has address 9.67.101.1, ifidx 3',\n",
       " '03/22 08:51:06 error   :...read_physical_netif: index #5, interface CTCD2 has address 9.67.117.98, ifidx 5',\n",
       " '03/22 08:51:06 warning :...read_physical_netif: index #3, interface LINK12 has address 9.67.101.1, ifidx 3',\n",
       " '03/22 08:51:06 warning :...read_physical_netif: index #6, interface LOOPBACK has address 127.0.0.1, ifidx 0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd75bd-a398-4b45-b58e-553f86e090a1",
   "metadata": {},
   "source": [
    "### collect() - RDD Actions\n",
    "\n",
    "`collect()` return all the elements of the dataset as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "666cfd4f-4648-4ed6-b744-5e368d1f3d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331136ba-1e2b-448f-b0db-ec6cd4314018",
   "metadata": {},
   "source": [
    "### take(N) - RDD Actions\n",
    "\n",
    "`take(N)` returns an array with the first N elements of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ca74a42-089c-4e3f-b2bb-b717456d9c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_map.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe59fd-1d54-40ec-9b80-d83b174f0e0b",
   "metadata": {},
   "source": [
    "### first() - RDD Actions\n",
    "\n",
    "`first()` prints the first element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2114e823-6a61-4f5d-9b9a-f479433cf550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_map.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dd5bf-82e0-425a-b649-b7a66e5b115b",
   "metadata": {},
   "source": [
    "### count() - RDD Actions\n",
    "\n",
    "`count()` return the number of elements in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5a8c53b-e702-473d-9928-86226e7ff601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_map.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d87aec-c070-4897-a16f-2351c07b64ff",
   "metadata": {},
   "source": [
    "## Ex. 4 - Map and Collect\n",
    "\n",
    "The main method with which you can manipulate data in PySpark is using `map()`. The `map()` transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. In this simple exercise, you'll use `map()` transformation to cube each number of the `numbRDD` RDD that you've created earlier. Next, you'll store all the elements in a variable and finally print the output.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create `map()` transformation that cubes all of the numbers in `numbRDD`.\n",
    "2. Collect the results in a `numbers_all` variable.\n",
    "3. Print the output from `numbers_all` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "049b9dd6-1611-40d4-838c-9bd4e888d1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "numbRDD = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "print(numbers_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acbbbc-63f9-44b0-a836-0914a6aec454",
   "metadata": {},
   "source": [
    "## Ex. 5 - Filter and Count\n",
    "The RDD transformation `filter()` returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword. For this exercise, you'll filter out lines containing keyword Spark from fileRDD RDD which consists of lines of text from the `sample_text.md` file. Next, you'll count the total number of lines containing the keyword Spark and finally print the first 4 lines of the filtered RDD.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create `filter()` transformation to select the lines containing the keyword Spark.\n",
    "2. How many lines in `fileRDD_filter` contain the keyword Spark?\n",
    "3. Print the first four lines of the resulting RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "144b68d1-60e7-4873-8f64-e8050bee6956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines with the keyword Spark is 7\n",
      "\n",
      "First 4 lines of filtered file:\n",
      "(1) - Examples for Learning Spark\n",
      "(2) - Examples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file\n",
      "(3) - These examples have been updated to run against Spark 1.3 so they may\n",
      "(4) - be slightly different than the versions in your copy of \"Learning Spark\".\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "file_path = 'data-sources/sample_text.md'\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "print(\"\\nFirst 4 lines of filtered file:\")\n",
    "for i, line in enumerate(fileRDD_filter.take(4)):\n",
    "  print(f'({i+1}) - {line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec6f93-1b75-4090-8f27-26407b01347e",
   "metadata": {},
   "source": [
    "## Pair RDDs in PySpark\n",
    "\n",
    "Pair RDD is a special data structure to work with key/value pairs datasets. Pair RDD: Key is the identifier and value is the data.\n",
    "\n",
    "### Creating pair RDDs - From a list of key-value tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "512dc8e0-1f99-462e-bb6a-54762f246e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data: [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
      "Keys: ['Sam', 'Mary', 'Peter']\n",
      "Values: [23, 34, 25]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "# Reviewing the created pair RDDs\n",
    "print(f'''\n",
    "Data: {pairRDD_tuple.collect()}\n",
    "Keys: {pairRDD_tuple.keys().collect()}\n",
    "Values: {pairRDD_tuple.values().collect()}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250de1a0-8208-4063-a573-5b6c3c3f2eac",
   "metadata": {},
   "source": [
    "### Creating pair RDDs - From a regular RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bb5b189-cc2f-46ca-97c0-6e3b88915cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data: [('Sam', '23'), ('Mary', '34'), ('Peter', '25')]\n",
      "Keys: ['Sam', 'Mary', 'Peter']\n",
      "Values: ['23', '34', '25']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "\n",
    "# Reviewing the created pair RDDs\n",
    "print(f'''\n",
    "Data: {pairRDD_RDD.collect()}\n",
    "Keys: {pairRDD_RDD.keys().collect()}\n",
    "Values: {pairRDD_RDD.values().collect()}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20a650-de12-44dc-9979-25de95e66765",
   "metadata": {},
   "source": [
    "### reduceByKey(func) - Transformations on pair RDDs\n",
    "\n",
    "- `reduceByKey()` transformation combines values with the same key.\n",
    "- It runs parallel operations for each key in the dataset.\n",
    "- It is a transformation and not action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eea33d9-e5e3-4f86-8982-ca4ef97af197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Neymar', 22), ('Ronaldo', 34), ('Messi', 147)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), \n",
    "                             (\"Messi\", 24), (\"Messi\", 100), ])\n",
    "\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x, y : x + y)\n",
    "pairRDD_reducebykey.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a8010-df0e-41c8-a16a-8d4b77306929",
   "metadata": {},
   "source": [
    "### groupByKey() - Transformations on pair RDDs\n",
    "\n",
    "- `sortByKey()` operation orders pair RDD by key.\n",
    "- It returns an RDD sorted by key in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e70c02a-e177-4db0-912b-004d5d0021f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(147, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3514c2-37d4-462a-a5f7-4cfe8077dc66",
   "metadata": {},
   "source": [
    "### sortByKey() - Transformations on pair RDDs\n",
    "\n",
    "- `groupByKey()` groups all the values with the same key in the pair RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "734218bb-6add-44ab-bc23-542295aa60c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK ['LHR']\n",
      "US ['JFK', 'SFO']\n",
      "FR ['CDG']\n"
     ]
    }
   ],
   "source": [
    "airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US\", \"SFO\")]\n",
    "regularRDD = sc.parallelize(airports)\n",
    "\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6854890-0a39-4f12-a45b-dbb23d868fed",
   "metadata": {},
   "source": [
    "### join() - Transformations on pair RDDs\n",
    "\n",
    "- `join()` transformation joins the two pair RDDs based on their key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "775196a2-dbf3-43e2-bebe-6218fbfcb8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Neymar', (24, 120)), ('Messi', (34, 100)), ('Ronaldo', (32, 80))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])\n",
    "\n",
    "RDD1.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f78803-ca7a-4983-9f13-2f3194c129ce",
   "metadata": {},
   "source": [
    "## Ex. 6 - ReduceBykey and Collect\n",
    "\n",
    "One of the most popular pair RDD transformations is `reduceByKey()` which operates on key, value (k,v) pairs and merges the values for each key. In this exercise, you'll first create a pair RDD from a list of tuples, then combine the values with the same key and finally print out the result.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a pair RDD named Rdd with tuples `(1,2),(3,4),(3,6),(4,5)`.\n",
    "2. Transform the `Rdd` with `reduceByKey()` into a pair RDD `Rdd_Reduced` by adding the values with the same key.\n",
    "3. Collect the contents of pair RDD `Rdd_Reduced` and iterate to print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00cbd64c-6876-4b2e-aa5b-df0eada93cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6), (4, 5)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "Rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "843f6853-c4be-418b-ac87-1a1d2f319e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10), (4, 5)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "Rdd_Reduced.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cf7ca25-6686-4a94-9bdf-166c4e897c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1ec66-439a-4dd7-9bcd-276d597fe5b8",
   "metadata": {},
   "source": [
    "## Ex. 7 - SortByKey and Collect\n",
    "\n",
    "Many times it is useful to sort the pair RDD based on the key (for example word count which you'll see later in the track). In this exercise, you'll sort the pair RDD `Rdd_Reduced` that you created in the previous exercise into descending order and print the final output.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Sort the `Rdd_Reduced` RDD using the key in descending order.\n",
    "2. Collect the contents and iterate to print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23db7b17-6c69-47aa-9133-22bee85e647b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10), (4, 5)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing the data to sort\n",
    "Rdd_Reduced.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "888a6233-6c8a-4477-b7d7-3b7174c46fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 5), (3, 10), (1, 2)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "Rdd_Reduced_Sort.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4c51efe-90ed-4a67-8fbe-da63a3a4b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the result and retrieve all the elements of the RDD\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b81593b-7846-48b1-aaa4-3b9abd436d76",
   "metadata": {},
   "source": [
    "## Advanced RDD Actions\n",
    "\n",
    "### reduce() action\n",
    "\n",
    "- `reduce(func)` action is used for aggregating the elements of a regular RDD.\n",
    "- The function should be commutative (changing the order of the operands does not change\n",
    "the result) and associative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d89aaf9c-1f3c-4d6d-a7b8-3e82bdb6d692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,3,4,6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e59312f-fb29-47fb-b84c-68c645e40feb",
   "metadata": {},
   "source": [
    "### saveAsTextFile() action\n",
    "\n",
    "- `saveAsTextFile()` action saves RDD into a text file inside a directory with each partition as\r\n",
    "a separate fil.\n",
    "- `coalesce()` method can be used to save RDD as a single text filee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b32c4fc9-b9e2-484c-a256-515ce56f8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to remove previous saved data.\n",
    "output_file_path = \"spark-warehouse/tempSampleText\"\n",
    "output_single_file_path = \"spark-warehouse/tempSampleText_single\"\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_file_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_single_file_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3466f205-e422-41c5-9796-671a074f6f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark-warehouse/tempSampleText\\\\part-00000',\n",
       " 'spark-warehouse/tempSampleText\\\\part-00001',\n",
       " 'spark-warehouse/tempSampleText\\\\_SUCCESS']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "file_path = 'data-sources/sample_text.md'\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "fileRDD.saveAsTextFile(output_file_path)\n",
    "glob.glob(f'{output_file_path}/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80d04c27-4fd4-461d-8ae7-859a70d2a024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark-warehouse/tempSampleText_single\\\\part-00000',\n",
       " 'spark-warehouse/tempSampleText_single\\\\_SUCCESS']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileRDD.coalesce(1).saveAsTextFile(output_single_file_path)\n",
    "glob.glob(f'{output_single_file_path}/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236f845-1e9e-493a-9a92-f455c9348ec9",
   "metadata": {},
   "source": [
    "### countByKey() - Action Operations on pair RDDs\n",
    "\n",
    "- `countByKey()` only available for type `(K, V)`.\n",
    "- `countByKey()` action counts the number of elements for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e15b423-2a3a-4162-ba42-58100a7a04e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 2\n",
      "b 1\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "\n",
    "for kee, val in rdd.countByKey().items():\n",
    "    print(kee, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71879f-4027-4f8e-958e-a24368cc3fcb",
   "metadata": {},
   "source": [
    "### collectAsMap() - Action Operations on pair RDDs\n",
    "\n",
    "- `collectAsMap()` return the key-value pairs in the RDD as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d5e9d6e-c407-4895-b16e-896c3e185879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 3: 4}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976ca45-d509-4127-bc24-6973c22a7b3d",
   "metadata": {},
   "source": [
    "## Ex. 8 - CountingBykeys\n",
    "\n",
    "For many datasets, it is important to count the number of keys in a key/value dataset. For example, counting the number of countries where the product was sold or to show the most popular baby names. In this simple exercise, you'll use the Rdd that you created earlier and count the number of unique keys in that pair RDD.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. `countByKeyand` assign the result to a variable `total`.\n",
    "2. What is the type of `total`?\n",
    "3. Iterate over the `total` and print the keys and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06be29d1-e7a1-4ae7-b569-7badad96a999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6), (4, 5)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "Rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8908b8bc-6572-4adb-9224-0e6936a83c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n",
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "# Count the unique keys\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b53782-86e1-4058-847b-bd4ec47fd4fe",
   "metadata": {},
   "source": [
    "## Ex. 9 - Create a base RDD and transform it\n",
    "\n",
    "The volume of unstructured data (log lines, images, binary files) in existence is growing dramatically, and PySpark is an excellent framework for analyzing this type of data through RDDs. In this 3 part exercise, you will write code that calculates the most common words from [Complete Works of William Shakespeare](data-sources/Complete_Shakespeare.txt).\n",
    "\n",
    "Here are the brief steps for writing the word counting program:\n",
    "\n",
    "- Create a base RDD from `Complete_Shakespeare.txt` file.\n",
    "- Use RDD transformation to create a long list of words from each element of the base RDD.\n",
    "- Remove stop words from your data.\n",
    "- Create pair RDD where each element is a pair tuple of `('w', 1)`\n",
    "- Group the elements of the pair RDD by key (word) and add up their values.\n",
    "- Swap the keys (word) and values (counts) so that keys is count and value is the word.\n",
    "- Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\n",
    "\n",
    "In this first exercise, you'll create a base RDD from `Complete_Shakespeare.txt` file and transform it to create a long list of words.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a RDD called baseRDD that reads lines from file_path.\n",
    "2. Transform the baseRDD into a long list of words and create a new splitRDD.\n",
    "3. Count the total number words in splitRDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "593b41ea-9f5e-4d2f-9a11-d23aadc6ecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 128576\n"
     ]
    }
   ],
   "source": [
    "# Create a baseRDD from the file path\n",
    "file_path = 'data-sources/Complete_Shakespeare.txt'\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c4123-00ce-4a91-99b3-6590fb83717f",
   "metadata": {},
   "source": [
    "## Ex. 10 - Remove stop words and reduce the dataset\n",
    "\n",
    "In this exercise you'll remove stop words from your data. Stop words are common words that are often uninteresting, for example, \"I\", \"the\", \"a\" etc. You can remove many obvious stop words with a list of your own. But for this exercise, you will just remove the stop words from a curated list `stop_words` provided to you in your environment.\n",
    "\n",
    "After removing stop words, you'll create a pair RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, pair RDD is composed of `(w, 1)` where `w` is for each word in the RDD and `1` is a number. Finally, you'll combine the values with the same key from the pair RDD to count the number of occurrences of each word.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Convert the words in `splitRDD` in lower case and then remove stop words from `stop_words` curated list. Think carefully about which function to use here.\n",
    "2. Create a pair RDD tuple containing the word and the number 1 from each word element in `splitRDD`.\n",
    "3. Get the count of the number of occurrences of each word (word frequency) in the pair RDD. Use a transformation which operates on key, value `(k,v)` pairs. Think carefully about which function to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd090183-324d-4b12-94ae-e8fcc7e3ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "              'you', 'your', 'yours', 'yourself', 'yourselves', 'he', \n",
    "              'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', \n",
    "              'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "              'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "              'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "              'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    "              'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "              'with', 'about', 'against', 'between', 'into', 'through', \n",
    "              'during', 'before', 'after', 'above', 'below', 'to', 'from', \n",
    "              'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "              'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "              'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
    "              'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', \n",
    "              'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e685a00-80ef-445c-bdfb-c78c2794574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total words in text: 128576\n",
      "Total words after removing stop words: 73305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the words in lower case and remove stop words from the stop_words curated list\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "print(f'''\n",
    "Total words in text: {splitRDD.count()}\n",
    "Total words after removing stop words: {splitRDD_no_stop.count()}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d418a75-3953-4871-a158-e908f47384f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 elements:\n",
      "[('Project', 1), ('Gutenberg', 1), ('EBook', 1), ('Complete', 1), ('Works', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "print(f'''\n",
    "First 5 elements:\n",
    "{splitRDD_no_stop_words.take(5)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "053a4d68-975b-47e5-97f4-a0fc2ca33b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 elements:\n",
      "[('Gutenberg', 7), ('EBook', 1), ('Complete', 3), ('Works', 3), ('Shakespeare', 12)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(f'''\n",
    "First 5 elements:\n",
    "{resultRDD.take(5)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc87ea-0d34-4215-9342-4da816233765",
   "metadata": {},
   "source": [
    "## Ex. 11 - Print word frequencies\n",
    "\n",
    "After combining the values (counts) with the same key (word), in this exercise, you'll return the first 10 word frequencies. You could have retrieved all the elements at once using `collect()`, but it is bad practice and not recommended. RDDs can be huge: you may run out of memory and crash your computer..\n",
    "\n",
    "What if we want to return the top 10 words? For this, first you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. Right now, `result_RDD` has key as element 0 and value as element 1. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count). This way it is easy to sort the RDD based on the key rather than using `sortByKey` operation in PySpark. Finally, you'll return the top 10 words based on their frequencies from the sorted RDD.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Print the first 10 words and their frequencies from the `resultRDD` RDD.\n",
    "2. Swap the keys and values in the `resultRDD`.\n",
    "3. Sort the keys according to descending order.\n",
    "4. Print the top 10 most frequent words and their frequencies from the sorted RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac681519-77a5-4e28-a0ec-e0cebed1a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gutenberg', 7)\n",
      "('EBook', 1)\n",
      "('Complete', 3)\n",
      "('Works', 3)\n",
      "('Shakespeare', 12)\n",
      "('eBook', 2)\n",
      "('use', 38)\n",
      "('anyone', 1)\n",
      "('cost', 8)\n",
      "('almost', 25)\n"
     ]
    }
   ],
   "source": [
    "# Display the first 10 words and their frequencies from the input RDD\n",
    "for word in resultRDD.take(10):\n",
    "\tprint(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1bf1f73-3e19-46d9-b0e0-4ff000d5d8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 'Gutenberg'),\n",
       " (1, 'EBook'),\n",
       " (3, 'Complete'),\n",
       " (3, 'Works'),\n",
       " (12, 'Shakespeare'),\n",
       " (2, 'eBook'),\n",
       " (38, 'use'),\n",
       " (1, 'anyone'),\n",
       " (8, 'cost'),\n",
       " (25, 'almost')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap the keys and values from the input RDD\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "resultRDD_swap.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72314b43-d07d-438d-90b9-b83444ed7769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(650, 'thou'),\n",
       " (574, 'thy'),\n",
       " (393, 'shall'),\n",
       " (311, 'would'),\n",
       " (295, 'good'),\n",
       " (286, 'thee'),\n",
       " (273, 'love'),\n",
       " (269, 'Enter'),\n",
       " (254, \"th'\"),\n",
       " (225, 'make')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "resultRDD_swap_sort.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f0d59ff-8d78-4b2c-ba97-f0a61cbd65d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou: 650 times\n",
      "thy: 574 times\n",
      "shall: 393 times\n",
      "would: 311 times\n",
      "good: 295 times\n",
      "thee: 286 times\n",
      "love: 273 times\n",
      "Enter: 269 times\n",
      "th': 254 times\n",
      "make: 225 times\n"
     ]
    }
   ],
   "source": [
    "# Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{}: {} times\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc8692-3132-46b7-a733-965e47df680d",
   "metadata": {},
   "source": [
    "## Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4b3324b-a307-4050-aa22-06cd52cf6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65789758-e047-453a-a2da-0951d767c478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
